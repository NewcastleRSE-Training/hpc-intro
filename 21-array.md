---
title: Parallelising with Job Arrays.
teaching: 15
exercises: 5
---



::::::::::::::::::::::::::::::::::::::: objectives

- Prepare a job submission script for an array job.

::::::::::::::::::::::::::::::::::::::::::::::::::

:::::::::::::::::::::::::::::::::::::::: questions

- What are job arrays?
- What benefit does job arrays bring?
- What type of jobs would benefit from job arrays?

::::::::::::::::::::::::::::::::::::::::::::::::::

Parallel computing is a technique used to divide big tasks into smaller ones that can be solved simultaneously. Parallelism can be accomplished in different ways and it depends on the tasks that needs doing as well as the algorithms implemented to perform these tasks. 



The simplest form of parallelism is to run the same job multiple times with different inputs. A more advanced form of parallelism is MPI, which is covered in the next episode. It is possible to use both these methods at the same time, using an array job to run multiple instances of software, which itself uses MPI.

cd training directory
mkdir username
cd username


Download the word frequency script
wget https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/word-freq.sh

write a small file to test our script

```bash
[user@cometlogin01(comet) ~] nano test-data.txt
[user@cometlogin01(comet) ~] cat test-data.txt
```

```bash
This is a small file - it will be very useful for trying out our script.
Some words are repeated in this file
- we can look for repeated words
and count them (to see which words are repeated most often).
```

Run at command line with a small test file (/nobackup/proj/comet_training/ArrayJob/test-data.txt)


Write a batch script to call the word-freq.sh as single job and run using slurm (job_single_word-freq.sh) 
or download it from https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/job_single_word-freq.sh

Download the data using https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/make-data.sh

Update the batch sript to run on the first file of 4 text files 
named data.1 data.2 data.3 data.4 
downloaded from the Guteberg project (in /nobackup/proj/comet_training/ArrayJob/ - see make-data.sh to download again)

Write a batch script to call the word-freq.sh as an array job with 4 parallel jobs to process all 4 text files (job_array_word-freq.sh)
or download it from https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/job_array_word-freq.sh

Check slurm.out files to see how quickly the jobs completed

```bash
#!/bin/bash
#SBATCH --partition=short_free
#SBATCH --job-name=
#SBATCH --nodes=1
#SBATCH --tasks=1
#SBATCH --cpus-per-task=1

# Do a word frequency analysis of the collected works of Shakespeare

DATA_FILE=data.1

echo "Starting word frequency analysis of $DATA_FILE"
echo "=============================================="

time cat $DATA_FILE | \
	sed s'/\ /\n/g' | \
	tr -c -d "[A-Za-z\n]" | \
	tr [A-Z] [a-z] | \
	sort | \
	strings -n 1 | \
	uniq -c | \
	sort -n > data.out

echo "====================================="
echo "Completed word analysis of $DATA_FILE"
```

:::::::::::::::::::::::::::::::::::::::: keypoints

- Parallel computing allows applications to 

::::::::::::::::::::::::::::::::::::::::::::::::::
